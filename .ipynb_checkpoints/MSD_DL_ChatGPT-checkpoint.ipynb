{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10f2ae93-2a61-4b02-8b6e-96cc9aa1ed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 1.5442e-04 - val_loss: 0.0032\n",
      "Epoch 2/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 5.3987e-06 - val_loss: 0.0025\n",
      "Epoch 3/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 5.9825e-06 - val_loss: 0.0027\n",
      "Epoch 4/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 4.7690e-06 - val_loss: 0.0020\n",
      "Epoch 5/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 6.1556e-06 - val_loss: 0.0018\n",
      "Epoch 6/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 4.8919e-06 - val_loss: 0.0012\n",
      "Epoch 7/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 5.3359e-06 - val_loss: 0.0023\n",
      "Epoch 8/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 4.2392e-06 - val_loss: 0.0014\n",
      "Epoch 9/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 3.2998e-06 - val_loss: 0.0020\n",
      "Epoch 10/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 4.5847e-06 - val_loss: 0.0016\n",
      "Epoch 11/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.9952e-06 - val_loss: 8.8704e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 3.1527e-06 - val_loss: 0.0013\n",
      "Epoch 13/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 2.7744e-06 - val_loss: 0.0016\n",
      "Epoch 14/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 2.8074e-06 - val_loss: 9.5122e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 2.7452e-06 - val_loss: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.3742e-06 - val_loss: 0.0012\n",
      "Epoch 17/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.1782e-06 - val_loss: 9.6640e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.4473e-06 - val_loss: 0.0017\n",
      "Epoch 19/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.0213e-06 - val_loss: 0.0015\n",
      "Epoch 20/20\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.9383e-06 - val_loss: 0.0024\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Model Evaluation\n",
      "MAE: 46.03\n",
      "MSE: 3636.39\n",
      "RMSE: 60.30\n",
      "R2 Score: 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Forecast future prices\\nfuture_prices = []\\ncurrent_sequence = X_test[-1]  # Start with the last known sequence\\n\\nfor _ in range(10):\\n    predicted_price = model.predict(current_sequence.reshape(1, seq_length, 1))[0, 0]\\n    future_prices.append(predicted_price)\\n    current_sequence = np.roll(current_sequence, -1)\\n    current_sequence[-1] = predicted_price  # Update sequence with prediction\\n\\n# Convert prices back to original scale\\nfuture_prices = scaler.inverse_transform(np.array(future_prices).reshape(-1, 1)).flatten()\\n\\n# Print predictions\\nfor date, price in zip(predict_dates, future_prices):\\n    print(f\"Predicted stock price for {date.strftime(\\'%Y-%m-%d\\')}: ${price:.2f}\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"Datasetscomp1/TSLA.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[\"Close\"] = scaler.fit_transform(df[\"Close\"].values.reshape(-1, 1))\n",
    "\n",
    "# Create sequences for LSTM/GRU\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        labels.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "seq_length = 50  # Number of past days to use for prediction\n",
    "close_prices = df[\"Close\"].values\n",
    "X, y = create_sequences(close_prices, seq_length)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Reshape input for LSTM/GRU (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build LSTM/GRU Model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "    GRU(50, return_sequences=False),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R2 Score: {r2:.2f}\")\n",
    "\n",
    "\"\"\"\n",
    "# Generate future dates\n",
    "start_date = datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\")\n",
    "predict_dates = [start_date + timedelta(days=i) for i in range(10)]\n",
    "\n",
    "# Forecast future prices\n",
    "future_prices = []\n",
    "current_sequence = X_test[-1]  # Start with the last known sequence\n",
    "\n",
    "for _ in range(10):\n",
    "    predicted_price = model.predict(current_sequence.reshape(1, seq_length, 1))[0, 0]\n",
    "    future_prices.append(predicted_price)\n",
    "    current_sequence = np.roll(current_sequence, -1)\n",
    "    current_sequence[-1] = predicted_price  # Update sequence with prediction\n",
    "\n",
    "# Convert prices back to original scale\n",
    "future_prices = scaler.inverse_transform(np.array(future_prices).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Print predictions\n",
    "for date, price in zip(predict_dates, future_prices):\n",
    "    print(f\"Predicted stock price for {date.strftime('%Y-%m-%d')}: ${price:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "164c634a-6572-4daf-8e20-cc0bc53e1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 2324, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 0.027581\n",
      "XGBoost Model Evaluation\n",
      "MAE: 485.33\n",
      "MSE: 333579.57\n",
      "RMSE: 577.56\n",
      "R2 Score: -2.40\n",
      "\n",
      "LightGBM Model Evaluation\n",
      "MAE: 483.53\n",
      "MSE: 331755.95\n",
      "RMSE: 575.98\n",
      "R2 Score: -2.38\n",
      "\n",
      "Future Predictions (XGBoost)\n",
      "Predicted stock price for 2023-01-01: $71.38\n",
      "Predicted stock price for 2023-01-02: $68.09\n",
      "Predicted stock price for 2023-01-03: $66.44\n",
      "Predicted stock price for 2023-01-04: $62.84\n",
      "Predicted stock price for 2023-01-05: $60.10\n",
      "Predicted stock price for 2023-01-06: $57.39\n",
      "Predicted stock price for 2023-01-07: $55.04\n",
      "Predicted stock price for 2023-01-08: $53.10\n",
      "Predicted stock price for 2023-01-09: $52.58\n",
      "Predicted stock price for 2023-01-10: $52.58\n",
      "\n",
      "Future Predictions (LightGBM)\n",
      "Predicted stock price for 2023-01-01: $73.27\n",
      "Predicted stock price for 2023-01-02: $72.13\n",
      "Predicted stock price for 2023-01-03: $71.97\n",
      "Predicted stock price for 2023-01-04: $71.82\n",
      "Predicted stock price for 2023-01-05: $71.82\n",
      "Predicted stock price for 2023-01-06: $71.82\n",
      "Predicted stock price for 2023-01-07: $71.82\n",
      "Predicted stock price for 2023-01-08: $71.82\n",
      "Predicted stock price for 2023-01-09: $71.96\n",
      "Predicted stock price for 2023-01-10: $71.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1119f\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"Datasetscomp1/TSLA.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[\"Close\"] = scaler.fit_transform(df[\"Close\"].values.reshape(-1, 1))\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        labels.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "seq_length = 50  # Number of past days to use for prediction\n",
    "close_prices = df[\"Close\"].values\n",
    "X, y = create_sequences(close_prices, seq_length)\n",
    "\n",
    "# Reshape data for XGBoost/LightGBM\n",
    "X = X.reshape(X.shape[0], -1)  # Flatten sequences\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Train XGBoost Model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Train LightGBM Model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "lgb_pred = lgb_model.predict(X_test)\n",
    "\n",
    "xgb_pred = scaler.inverse_transform(xgb_pred.reshape(-1, 1))\n",
    "lgb_pred = scaler.inverse_transform(lgb_pred.reshape(-1, 1))\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate evaluation metrics for XGBoost\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(xgb_mse)\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "# Calculate evaluation metrics for LightGBM\n",
    "lgb_mae = mean_absolute_error(y_test, lgb_pred)\n",
    "lgb_mse = mean_squared_error(y_test, lgb_pred)\n",
    "lgb_rmse = np.sqrt(lgb_mse)\n",
    "lgb_r2 = r2_score(y_test, lgb_pred)\n",
    "\n",
    "print(\"XGBoost Model Evaluation\")\n",
    "print(f\"MAE: {xgb_mae:.2f}\")\n",
    "print(f\"MSE: {xgb_mse:.2f}\")\n",
    "print(f\"RMSE: {xgb_rmse:.2f}\")\n",
    "print(f\"R2 Score: {xgb_r2:.2f}\")\n",
    "\n",
    "print(\"\\nLightGBM Model Evaluation\")\n",
    "print(f\"MAE: {lgb_mae:.2f}\")\n",
    "print(f\"MSE: {lgb_mse:.2f}\")\n",
    "print(f\"RMSE: {lgb_rmse:.2f}\")\n",
    "print(f\"R2 Score: {lgb_r2:.2f}\")\n",
    "\n",
    "# Generate future dates\n",
    "start_date = datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\")\n",
    "predict_dates = [start_date + timedelta(days=i) for i in range(10)]\n",
    "\n",
    "# Forecast future prices using XGBoost\n",
    "future_prices_xgb = []\n",
    "current_sequence = X_test[-1]\n",
    "for _ in range(10):\n",
    "    predicted_price = xgb_model.predict(current_sequence.reshape(1, -1))[0]\n",
    "    future_prices_xgb.append(predicted_price)\n",
    "    current_sequence = np.roll(current_sequence, -1)\n",
    "    current_sequence[-1] = predicted_price\n",
    "future_prices_xgb = scaler.inverse_transform(np.array(future_prices_xgb).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Forecast future prices using LightGBM\n",
    "future_prices_lgb = []\n",
    "current_sequence = X_test[-1]\n",
    "for _ in range(10):\n",
    "    predicted_price = lgb_model.predict(current_sequence.reshape(1, -1))[0]\n",
    "    future_prices_lgb.append(predicted_price)\n",
    "    current_sequence = np.roll(current_sequence, -1)\n",
    "    current_sequence[-1] = predicted_price\n",
    "future_prices_lgb = scaler.inverse_transform(np.array(future_prices_lgb).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Print predictions\n",
    "print(\"\\nFuture Predictions (XGBoost)\")\n",
    "for date, price in zip(predict_dates, future_prices_xgb):\n",
    "    print(f\"Predicted stock price for {date.strftime('%Y-%m-%d')}: ${price:.2f}\")\n",
    "\n",
    "print(\"\\nFuture Predictions (LightGBM)\")\n",
    "for date, price in zip(predict_dates, future_prices_lgb):\n",
    "    print(f\"Predicted stock price for {date.strftime('%Y-%m-%d')}: ${price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d321b54-dbe6-4896-8877-3147080d923f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
