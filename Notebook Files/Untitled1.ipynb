{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab633fa-510e-4b9a-8a96-6caac6fcda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse functions for various ML tasks\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ********* AUXILIARY FUNCTIONS ********\n",
    "# This avoids the errors that normally occur if you try to take the\n",
    "# log of a non-positive value.\n",
    "def safelog(x,base=2): return math.log(x,base) if x > 0 else 0\n",
    "\n",
    "# Convert a list of numbers into a list wherein all numbers sum\n",
    "# to 1 or, when geometric = True, to a list wherein the sum of\n",
    "# the SQUARES of all numbers is 1.\n",
    "\n",
    "def normalize_list(elems,geometric=False):\n",
    "    if geometric:\n",
    "        s = math.sqrt(sum(map((lambda x: x**2), elems)))\n",
    "    else:\n",
    "        s = sum(elems)\n",
    "    if s != 0:\n",
    "        return [elem / s for elem in elems]\n",
    "    else:\n",
    "        return n_of(len(elems),0)\n",
    "\n",
    "# *************************************\n",
    "\n",
    "# This takes in any list of NON-NEGATIVE numbers and a) converts it to\n",
    "# a probability distribution, b) computes the entropy of that distribution.\n",
    "# E.g. entropy([1,2,3,4]) => 1.846,  entropy([0.25]*4) => 2.0,\n",
    "# entropy([0.125]*8) => 3.0, entropy([1]*8) => 3.0, etc.\n",
    "\n",
    "def entropy(items, base=2):\n",
    "    distribution = normalize_list(items)\n",
    "    return -sum([d*safelog(d,base) for d in distribution])\n",
    "\n",
    "# A master set has been partitioned, so compute entropy of each subset and\n",
    "# then weight each entropy by the normalized size of the subset.  This is\n",
    "# the main entropy calculation in algorithms such as ID3 and C4.5.\n",
    "# NOTE:  The subsets should NOT be normalized; otherwise, each subset will\n",
    "# have the same sum and size-normalization will be useless.\n",
    "# E.g. For a 3-way partition into subsets representing two classes:\n",
    "#  partition_entropy(((4,1),(4,2),(2,3))) => 0.873.\n",
    "# For a 2-way partition into subsets representing 4 classes:\n",
    "#  partition_entropy(((5,0,0,1),(2,2,2,2))) => 1.4214, and\n",
    "# partition_entropy(((0,6,0,0),(8,0,0,0))) => 0.0\n",
    "# In this last example, (0,6,0,0) represents a subset where all 6 items\n",
    "# are in class 2, while none are in classes 1, 3 or 4.\n",
    "\n",
    "def partition_entropy(partition):\n",
    "    sizes = normalize_list([sum(sub) for sub in partition])\n",
    "    sub_entropies = [entropy(sub) for sub in partition]\n",
    "    return sum([s*e for s,e in zip(sizes,sub_entropies)])\n",
    "\n",
    "# This computes the Standard Deviation Reduction (SDR) for different partitions of the examples at any\n",
    "# node of a regression tree.  The classvals slot = values of the \"class\" for each case at the current node\n",
    "# of a regression tree.\n",
    "\n",
    "class RegTreeSplitter:\n",
    "\n",
    "    def __init__(self,classvals):\n",
    "        self.classvals = classvals # dict {case-id: class-value}\n",
    "        self.size = len(classvals)\n",
    "        cvals = list(self.classvals.values())\n",
    "        self.avg = np.average(cvals)\n",
    "        self.stdev = np.std(cvals)\n",
    "\n",
    "\n",
    "    # Input = a subset of keys.  Output = stdev of classvals associated with those keys\n",
    "    def subset_stdev(self,subset):\n",
    "        vals = [self.classvals[key] for key in subset]\n",
    "        if len(vals) > 0:\n",
    "            return np.std(vals)\n",
    "        else: return 0\n",
    "\n",
    "    # partition = list of subsets (of the keys)\n",
    "    def partition_stdev(self,partition):\n",
    "        subset_stdevs = [self.subset_stdev(ss) for ss in partition]\n",
    "        scaled_stdevs = [std*len(ss)/self.size for ss,std in zip(partition,subset_stdevs)]\n",
    "        return subset_stdevs, scaled_stdevs, sum(scaled_stdevs)\n",
    "\n",
    "    def stdev_reduction(self,partition):\n",
    "        stdevs, scaled_stdevs, pstdev = self.partition_stdev(partition)\n",
    "        sdr = self.stdev - pstdev\n",
    "        print('Subset Stdevs = ', stdevs)\n",
    "        print('Scaled Stdevs = ', scaled_stdevs)\n",
    "        print('StDev Reduction = ', sdr)\n",
    "        return sdr\n",
    "\n",
    "def regtest():\n",
    "    master = {'a':20.3 ,'b':20.0 ,'c':19.9 ,'d':20.3 ,'e':20.10 ,'f':20.15 ,'g': 20.0,'h': 19.80}\n",
    "    rts = RegTreeSplitter(master)\n",
    "    return rts\n",
    "\n",
    "# This performs classic linear regression: it finds the closed-form solution (W) when mean-squared-error (MSE)\n",
    "# is to be minimized.  Inputs = X (array of feature vectors, one per row), Y = target vector.\n",
    "# This is minimizing MSE = (Y - WX)**2 by taking d(MSE) / dW and setting it to zero.  The closed-form\n",
    "# solution (see my slides for linear-regression in AI-prog lectures (ML)) is:\n",
    "#  W = Y * transpose(X) * (transpose(X) * X)**(-1)\n",
    "\n",
    "def linreg(features,targets):\n",
    "    num_features = features.shape[1]\n",
    "    f2 = np.insert(features, 0, np.ones(features.shape[0]),axis=1)  # Add column of 1's to accommodate bias weight, w[0].\n",
    "    f2_trans = np.transpose(f2)\n",
    "    x2 = f2_trans @ f2  # NOte:  @ is shorthand for np.matmul\n",
    "    #w = np.matmul(np.matmul(f2_trans,targets),np.linalg.inv(x2))\n",
    "    w = np.matmul(np.linalg.inv(x2), np.matmul(f2_trans,targets))  # The formula given on Wikipedia\n",
    "    print('Shapes: w = {}, f2 = {}'.format(w.shape,f2.shape))\n",
    "    predictions = w @ f2_trans\n",
    "    print('Predictions = {}'.format(predictions))\n",
    "    return w, sum(np.square(targets - predictions))\n",
    "\n",
    "def lint():\n",
    "    x = np.array(range(1,11)).reshape(5, 2)  # 5 cases of size 2\n",
    "    y = 11 + 3 * x[:, 0] + 5 * x[:, 1]  # weights = (11, 3, 5).  Linreg should FIND these !!\n",
    "    return x,y, linreg(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0bd0a1a-3c07-4fdf-a363-97adb33261a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1*8,2*8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd8b5e-9c26-45f3-8214-53c946e8b8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb83301-8c3c-4e5e-a13a-bb9310c35732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b463c54-9072-4935-857d-cf84075d5eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9512050593046015"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_entropy(((2, 3),(2, 3),(4, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b42d81cf-bb0c-4ad1-97c4-ccd125d6bfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9868178311574916"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_entropy(((5, 6),(3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72cf71d-a759-4e5d-ad63-a690cba24ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544340029249649"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_entropy(((5, 3),(3, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b59cca4-5b62-4924-bc19-0b33413f73c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7790245904193847"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_entropy(((1, 3, 2),(2, 3, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7050ceb0-616c-4f38-b7c0-bd8f710b0599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.498385528189818"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_entropy(((2, 2, 1),(2, 3, 1),(2, 1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051a168-2f12-45ee-ba24-8e65200253e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
